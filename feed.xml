<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://explain.yshui.dev/feed.xml" rel="self" type="application/atom+xml" /><link href="https://explain.yshui.dev/" rel="alternate" type="text/html" /><updated>2020-09-29T13:33:11-05:00</updated><id>https://explain.yshui.dev/feed.xml</id><title type="html">Explain Algorithms</title><subtitle>Computer algorithms in human readable format</subtitle><entry><title type="html">How you could have come up with Paxos yourself</title><link href="https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html" rel="alternate" type="text/html" title="How you could have come up with Paxos yourself" /><published>2020-09-20T00:00:00-05:00</published><updated>2020-09-20T00:00:00-05:00</updated><id>https://explain.yshui.dev/distributed%20system/2020/09/20/paxos</id><content type="html" xml:base="https://explain.yshui.dev/distributed%20system/2020/09/20/paxos.html">&lt;p&gt;In the field of computer science, the Paxos algorithm is notorious for how difficult it is to understand. I had to learn the Paxos algorithm in my distributed systems class. I even have “implemented” it by translating Leslie Lamport’s TLA+ to Python. But I didn’t understand it until much much later.&lt;/p&gt;

&lt;p&gt;Now I have a better understanding of Paxos than I used to, I want to explain it to other people. Not because I’d like to help people, rather, I find that explaining things is a very good way to find blind spots in my own understanding.&lt;/p&gt;

&lt;p&gt;So, where do we start? Personally, I dislike explanations that start with a step-by-step breakdown of the algorithm, followed by a proof of why those steps do what they claim to do. Instead, I much prefer to start with the problem the algorithm tries to solve, then iteratively come up with a solution together with the reader. So that’s what I am going to do. And now you understand the title.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Small disclaimer:&lt;/em&gt; The glossaries used in this article is different from what is commonly used for Paxos. I just picked the ones that made the most sense for my narrative.&lt;/p&gt;

&lt;h2 id=&quot;the-problem&quot;&gt;The problem&lt;/h2&gt;

&lt;p&gt;The distributed consensus problem is widely useful, so the reader probably doesn’t need to be motivated. Here I will just simply state the problem.&lt;/p&gt;

&lt;p&gt;There is a group of agents (let’s call them $\sc{CLIENT}s$), who want to choose a number among their selections. Any number is fine, as long as everyone agrees on the same number.&lt;/p&gt;

&lt;p&gt;Here, there are a few assumptions we will make to make this problem meaningful:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;All the agents - including but not limited to the $\sc{CLIENT}s$, as we will add more types of agents later - are well-behaved. Meaning they all execute the prescribed algorithms faithfully, and don’t maliciously try to trick other agents. (If you like jargons: Byzantine failures don’t occur.)&lt;/li&gt;
  &lt;li&gt;Agents can talk to each other by sending each other messages, but the messages they send to each other could take arbitrarily long before reaching their destination, and might get lost (but never altered).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The agents could also “fail”. However, failing is equivalent to all messages sent to/from that agent being lost forever. So whether we have this assumption or not won’t change the algorithm we come up with.&lt;/p&gt;

&lt;p&gt;Also, to not complicate things, we are only solving the “single-round” consensus problem in this article, meaning as the output of this algorithm, all of the $\sc{CLIENT}s$ will get a single number which they agree on.&lt;/p&gt;

&lt;h2 id=&quot;solution-searching-adventure&quot;&gt;Solution searching adventure&lt;/h2&gt;

&lt;h3 id=&quot;iteration-0&quot;&gt;Iteration 0&lt;/h3&gt;

&lt;p&gt;When trying to solve a complex problem such as this one, it’s usually a good idea to start by simplifying the problem.
As a start, let’s just ignore the need to be reliable entirely.&lt;/p&gt;

&lt;p&gt;If we throw reliability out the window, it should be easy to come up with a very simple solution: we add an agent (let’s call it $\sc{COORDINATOR}$).
The $\sc{CLIENTS}$ send whatever number they pick to the $\sc{COORDINATOR}$ in a $\sc{PROPOSAL}(client_i, x)$ message, where $x$ is the number
proposed by the $i$-th $\sc{CLIENT}$. The $\sc{COORDINATOR}$ picks an arbitrary proposal (say, $x’$),
and informs the other $\sc{CLIENT}s$ about this decision.
Specifically, the $\sc{COORDINATOR}$ will just reply with a $\sc{CHOSEN}(x’)$ message to all the $\sc{PROPOSAL}(\ldots)$ messages it has
received and will receive.&lt;/p&gt;

&lt;p&gt;If we assume no messages ever get lost, it is quite easy to see that every $\sc{CLIENT}$ will get a number. And because only one number is ever chosen, they will all get the same number.&lt;/p&gt;

&lt;p&gt;It is also easy to see why this solution is impractical: it has a single point of failure. Once the singular $\sc{COORDINATOR}$ fails, no further progress can be made.&lt;/p&gt;

&lt;h3 id=&quot;iteration-1&quot;&gt;Iteration 1&lt;/h3&gt;

&lt;p&gt;To improve this almost looks easy at first glance: just add more $\sc{COORDINATOR}s$!&lt;/p&gt;

&lt;p&gt;Sure, more $\sc{COORDINATOR}s$ would remove the single point of failure. However, if there are more than one $\sc{COORDINATOR}s$, they might individually make different decisions, which results in the $\sc{CLIENT}s$ having disagreement.&lt;/p&gt;

&lt;p&gt;What if we let the $\sc{COORDINATOR}s$ reach an agreement among themselves before responding? But wait, doesn’t that sound familiar? Having a group of agents reaching an agreement, that’s exactly what we added the $\sc{COORDINATOR}s$ to solve. We just made the problem cyclic.&lt;/p&gt;

&lt;p&gt;Let’s take a step back. Is there a way for the clients to reach an agreement without having the $\sc{COORDINATOR}s$ communicate with each other?&lt;/p&gt;

&lt;p&gt;In other words, among the decisions of the $\sc{COORDINATOR}s$, is there an deterministic algorithm to pick out a specific one that is robust against message losses?&lt;/p&gt;

&lt;p&gt;This might sound hard, but it’s actually quite simple: pick the decision that is backed by more than half of the $\sc{COORDINATOR}s$.&lt;/p&gt;

&lt;p&gt;There can’t be two decisions both with more than half of the $\sc{COORDINATOR}s$ backing them; and if a decision doesn’t have that many $\sc{COORDINATOR}s$ backing it, it won’t appear to have more backing $\sc{COORDINATOR}s$ through message losses.&lt;/p&gt;

&lt;p&gt;Since this approach resembles a majority vote, let’s call $\sc{COORDINATOR}$ decisions $\sc{VOTE}(coord_i, x)$ from now on, where $x$ is the number picked by the $i$-th $\sc{COORDINATOR}$. Each $\sc{COORDINATOR}$ has a single vote, because each of them only makes a single decision.&lt;/p&gt;

&lt;p&gt;Obviously, our solution cannot be infinitely reliable. If more than half of the $\sc{COORDINATOR}s$ went down, there will never be a majority reached. But this is already vastly better than our first solution, and the reliability scales with the number of $\sc{COORDINATOR}s$. So we will call it good enough.&lt;/p&gt;

&lt;p&gt;Sadly, this solution doesn’t actually work: there might not be a majority at all! For example, it’s possible that three of the proposals each get a third of the votes. We would have a stalemate in that case.&lt;/p&gt;

&lt;h3 id=&quot;iteration-2&quot;&gt;Iteration 2&lt;/h3&gt;

&lt;p&gt;Again, a solution seems straightforward: just try again in case of a stalemate.&lt;/p&gt;

&lt;p&gt;But then again, things aren’t that simple.&lt;/p&gt;

&lt;p&gt;First of all, the $\sc{COORDINATOR}s$ need to be made aware of a retry. Otherwise, because each $\sc{COORDINATOR}$ only has one vote, they won’t be able to vote again even if the $\sc{CLIENT}s$ retry.&lt;/p&gt;

&lt;p&gt;To do that, we attach an attempt id to all the messages sent. i.e. $\sc{PROPOSAL}(client_i, x)$ becomes $\sc{PROPOSAL}(\#attempt, client_i, x)$, and so forth. Each time a $\sc{CLIENT}$ retries, it bumps $\#attempt$ to the maximum $\#attempt$ it knows of plus 1. And the $\sc{COORDINATOR}s$ should only responds to messages with the most recent $\#attempt$.&lt;/p&gt;

&lt;p&gt;Hopefully the intent of the $\#attempt$ number is clear. (&lt;a href=&quot;https://github.com/yshui/explain-algorithms/issues/new&quot;&gt;Let me know&lt;/a&gt; if not.)&lt;/p&gt;

&lt;p&gt;Are we good now? Unfortunately, no. Consider this scenario:&lt;/p&gt;

&lt;p&gt;There were 2 clients. They proposed their numbers, the $\sc{COORDINATOR}$ voted on them and all agreed on a single number, $x_1$, all is good. But, all of the $\sc{VOTE}(\ldots)$ messages got lost on the way to $client_2$, while $client_1$ received all of the messages just fine. At this point, $client_1$ thought $x_1$ is the number, but $client_2$ went on to retry. The $\sc{COORDINATOR}s$ voted again, and got $x_2$. This time, all the messages sent to $client_1$ got lost.&lt;/p&gt;

&lt;p&gt;And behold, we got the two clients to disagree.&lt;/p&gt;

&lt;p&gt;There is an important insight to be had here. Whenever a $\sc{COORDINATOR}$, say $coord_i$, sends out a $\sc{VOTE}(\ldots, coord_i, x)$, there is a chance that some $\sc{CLIENT}$ would adopt $x$. If $coord_i$ ever sends out two votes with different $x$, there is a chance that some of the $\sc{CLIENT}s$ would disagree.&lt;/p&gt;

&lt;p&gt;In other words, once a $\sc{COORDINATOR}$ has revealed its vote, it has to stick to it.&lt;/p&gt;

&lt;p&gt;This seems to run contrary to our attempt: if the $\sc{COORDINATOR}s$ cannot change their votes, what’s the point of retrying? A stalemate will be a stalemate forever.&lt;/p&gt;

&lt;p&gt;Looks like we reached a dead end with this type of voting. It appears the problem stems from the fact that the $\sc{COORDINATOR}s$ have to commit to their votes.&lt;/p&gt;

&lt;p&gt;So, what if we introduce a form of non-commitment voting?&lt;/p&gt;

&lt;h3 id=&quot;iteration-3&quot;&gt;Iteration 3&lt;/h3&gt;

&lt;p&gt;Let’s explore this idea. Say, the $\sc{COORDINATOR}s$ could now send a $\sc{TENTATIVE}\sc{VOTE}(\#attempt, coord_i, x)$ message, to tentatively vote for $x$.&lt;/p&gt;

&lt;p&gt;Obviously, the $\sc{CLIENT}s$ couldn’t adopt $x$ right away. So what’s this vote good for?&lt;/p&gt;

&lt;p&gt;Ah, right, it could get us to a majority.&lt;/p&gt;

&lt;p&gt;It is correct that tentative votes don’t lead directly to an agreement among $\sc{CLIENT}s$, but it can show us when a majority has formed among the $\sc{COORDINATOR}s$.&lt;/p&gt;

&lt;p&gt;Once a $\sc{CLIENT}$ sees a majority tentative vote, it can then message the $\sc{COORDINATOR}s$ to ask for an actual vote. (Let’s call this message $\sc{PLEASE}\sc{VOTE}(\#attempt, client_i)$). Intuitively, the $\sc{COORDINATOR}s$ have to make the same vote in the actual vote as their tentative votes.&lt;/p&gt;

&lt;p&gt;If all goes well, we would get a majority and an agreement. If there is no majority, the $\sc{COORDINATOR}s$ won’t even start a vote, so they are free to change their mind. So the $\sc{CLIENT}s$ could start another attempt which might have a different outcome.&lt;/p&gt;

&lt;p&gt;What if things don’t go well? What if the $\sc{PLEASE}\sc{VOTE}$ messages weren’t received by some of the $\sc{COORDINATOR}s$?
In that case, some of the $\sc{COORDINATOR}s$ would have voted, and their decisions cannot be changed. That is to say, in all subsequent attempts, these $\sc{COORDINATOR}s$ will always vote for what they have voted for, whether it’s a tentative vote, or the actual vote. But that doesn’t create a problem for us. There was a majority in the tentative votes, and now we solidified part of the tentative votes. There is at least one way we can still reach a majority in the next round: everyone votes the same as they did in this round. And we can prove this inductively for all future rounds.&lt;/p&gt;

&lt;p&gt;From this, we can have a rough image of how the algorithm functions: as attempts are being made, more and more $\sc{COORDINATOR}s$ start to make up their mind which number they will commit to, while making sure a majority could still be reached. Eventually, after all the $\sc{COORDINATOR}s$ have made up their minds, by induction there must be a majority among them. From that point on, they will just repeatedly broadcast this decision to the $\sc{CLIENT}s$, until all the $\sc{CLIENT}s$ have got that message.&lt;/p&gt;

&lt;p&gt;And Viola, we have a working algorithm.&lt;/p&gt;

&lt;h3 id=&quot;the-actual-algorithm&quot;&gt;The actual algorithm&lt;/h3&gt;

&lt;p&gt;Let’s clean up our thoughts, and condense the description of our algorithm so it’s easy to understand.&lt;/p&gt;

&lt;p&gt;First, there is the $\#attempt$ number that is attached to every message. This number is bumped every time a new attempt is made. If a $\sc{CLIENT}$ sees a message with a $\#attempt$ bigger then its most recent $\#attempt$, it knows a new attempt has been initiated, so it would abort its current attempt and participate in the newer one. If a $\sc{COORDINATOR}$ sees a message with a $\#attempt$ smaller than the biggest $\#attempt$ it has ever seen, it would know that message is stale, so it will drop the message.&lt;/p&gt;

&lt;p&gt;With that out of the way, let’s describe what happens in an attempt.&lt;/p&gt;

&lt;p&gt;Each attempt can be split into two phases:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Phase 1:&lt;/strong&gt; The $\sc{CLIENT}s$ each send its $\sc{PROPOSAL}(\ldots)$ to the $\sc{COORDINATOR}s$. The $\sc{COORDINATOR}s$ reply with a $\sc{TENTATIVE}\sc{VOTE}(\ldots, x)$. Each $\sc{CLIENT}$ waits for the tentative votes until they reach a majority. If a majority is not reached, retry.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Phase 2:&lt;/strong&gt; If a majority is reached, the $\sc{CLIENT}s$ send $\sc{PLEASE}\sc{VOTE}$, and the $\sc{COORDINATOR}s$ actually vote. Their actual votes would be the same as their respective tentative votes. Each $\sc{CLIENT}$ waits for the votes until they reach a majority, and then adopt the majority number.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;back-to-paxos&quot;&gt;Back to Paxos&lt;/h2&gt;

&lt;p&gt;Our algorithm does look a bit different from the official Paxos. For one, the name of the agents are different. What we call $\sc{CLIENT}s$, Lamport calls $\sc{PROPOSER}s$; and $\sc{COORDINATOR}s$, $\sc{ACCEPTOR}s$.&lt;/p&gt;

&lt;p&gt;Besides that, there are protocol differences too.&lt;/p&gt;

&lt;p&gt;Firstly, the $\sc{COORDINATOR}s$ don’t have to send the $\sc{TENTATIVE}\sc{VOTE}(\ldots)$ to everyone, they just need to send it to the $\sc{CLIENT}$ they agree with. This way we won’t have every $\sc{CLIENT}s$ sending $\sc{PLEASE}\sc{VOTE}$ at the same time, that would be inefficient.&lt;/p&gt;

&lt;p&gt;After, we notice that the proposed number is unnecessarily sent multiple times in phase 1 and phase 2. The phase 1 is used to reach a majority, the proposed number is not actually important in that phase. So we remove the $x$ from $\sc{PROPOSAL}$; and in $\sc{TENTATIVE}\sc{VOTE}$, instead voting for a number, they vote for a $\sc{CLIENT}$, by sending the tentative vote only to that $\sc{CLIENTS}$. Finally, after a client received a majority of tentative votes, it sends a $\sc{PLEASE}\sc{VOTE}(x)$, so all the $\sc{COORDINATOR}s$ got that message will vote $x$. Of course, if a $\sc{COORDINATOR}$ has already voted in a previous round, it has to tell the $\sc{CLIENT}$, so it could pick the already voted $x$, otherwise its $\sc{PLEASE}\sc{VOTE}(x)$ will be wasted, as the $\sc{COORDINATOR}s$ couldn’t change their minds.&lt;/p&gt;

&lt;p&gt;(The modified algorithm has slightly better property. In our algorithm, we just make sure a majority is still possible after each attempt; in Paxos, each round the $\sc{COORDINATOR}s$ that vote will all vote for the same number.)&lt;/p&gt;

&lt;p&gt;With this little changes, we can map our algorithm back to Paxos:&lt;/p&gt;

&lt;p&gt;Agents:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\sc{CLIENT}$ =&amp;gt; $\sc{PROPOSER}$ (which makes proposals) and $\sc{LEARNER}$ (which adopts the resulting number)&lt;/li&gt;
  &lt;li&gt;$\sc{COORDINATOR}$ =&amp;gt; $\sc{ACCEPTOR}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Messages:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\sc{PROPOSAL}(\#attempt, client_i)$ =&amp;gt; $\sc{PREPARE}(\#attempt, client_i)$&lt;/li&gt;
  &lt;li&gt;$\sc{TENTATIVE}\sc{VOTE}(\#attempt, coord_i)$ =&amp;gt; $\sc{PROMISE}(\#attempt, coord_i)$&lt;/li&gt;
  &lt;li&gt;$\sc{PLEASE}\sc{VOTE}(\#attempt, client_i, x)$ =&amp;gt; $\sc{ACCEPT}(\#attempt, client_i, x)$&lt;/li&gt;
  &lt;li&gt;$\sc{VOTE}(\#attempt, coord_i, x)$ =&amp;gt; $\sc{ACCEPTED}(\#attempt, coord_i, x)$&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="Distributed system" /><summary type="html">In the field of computer science, the Paxos algorithm is notorious for how difficult it is to understand. I had to learn the Paxos algorithm in my distributed systems class. I even have “implemented” it by translating Leslie Lamport’s TLA+ to Python. But I didn’t understand it until much much later.</summary></entry></feed>